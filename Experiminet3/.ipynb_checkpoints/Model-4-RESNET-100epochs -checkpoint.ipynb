{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "audiodata='/Users/Divya Reddy/OneDrive/Desktop/Urbansound8k/Audio'\n",
    "metadata=pd.read_csv('/Users/Divya Reddy/OneDrive/Desktop/Urbansound8k/Metadata/UrbanSound8K.csv')\n",
    "file_viz = glob.glob('/Users/Divya Reddy/OneDrive/Desktop/Urbansound8k/Audio/fold1/*')\n",
    "print(\"shape of the metadata =\",metadata.shape)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {'0' : 'air_conditioner', '1' : 'car_horn', '2' : 'children_playing', '3' : 'dog_bark', '4' : 'drilling', \n",
    "                 '5' : 'engine_idling', '6' : 'gun_shot', '7' : 'jackhammer', '8' : 'siren', '9' : 'street_music'}\n",
    "pd.DataFrame(sorted(list(class_map.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [(class_map[label], [f for f in file_viz if f.split('-')[1] == label][0]) \n",
    "           for label in class_map.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sound_data(path, sr=22050):\n",
    "    data, fsr = sf.read(path)\n",
    "    data_22k = librosa.resample(data.T, fsr, sr)\n",
    "    if len(data_22k.shape) > 1:\n",
    "        data_22k = np.average(data_22k, axis=0)\n",
    "    return data_22k, sr\n",
    "sample_data = [(sample[0], get_sound_data(sample[1])) for sample in samples]\n",
    "[(sample[0], sample[1][0].shape) for sample in sample_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "\n",
    "        return (num_channels, sample_rate, bit_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:/Users/Divya Reddy/OneDrive/Desktop/Urbansound8k/audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfilehelper = WavFileHelper()\n",
    "\n",
    "audio_data = []\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join((file_path),'fold'+str(row[\"fold\"])+'/'+str(row[\"slice_file_name\"]))\n",
    "    data = wavfilehelper.read_file_properties(file_name)\n",
    "    audio_data.append(data)\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "audiodf = pd.DataFrame(audio_data, columns=['num_channels','sample_rate','bit_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_len = 174\n",
    "\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"classID\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf.feature[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into independent and dependent dataset\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "print(x_train.shape)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets,models,layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(10)\n",
    "model.build(input_shape = (None,40, 174, 1))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss of RESNET model\n",
    "\n",
    "metrics = history_relu.history\n",
    "plt.plot(history_relu.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['train_loss', 'test_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracy of RESNET Model\n",
    "\n",
    "plt.plot(history_relu.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history_relu.history['val_accuracy'], label='test_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
